# Cletho RAG MVP Project – Session Summary  
**Date:** 2025-08-25 & 2025-08-26  
**Facilitator:** ChatGPT  
**Participants:** Rick (Clear Thought Labs)

---

## 1. Next Steps

- Finalize and document the memory architecture for the Cletho RAG system.
- Design and scaffold the RAG corpus structure, including metadata tagging strategy.
- Begin draft architecture for the MVP RAG system using Supabase as the data backend.
- Establish a session journaling workflow using daily `.md` files.
- Configure persistent memory upload + retrieval pipeline using local Markdown session files.
- Define first-pass decision matrix step scripts and identify where RAG retrieval will occur.
- Explore classifiers for distinguishing user replies, user questions, and statements.
- Decide on embedding strategy for semantic search over session context.
- Discuss chunking vs summarization for RAG input construction.
- Clarify design tradeoffs between embedding raw session turns and distilled summaries.

---

## 2. Current Decisions

- **Session file format:** Each session summary will follow the 5-section Markdown structure.
- **Transcript inclusion:** Full session transcripts will be embedded directly in the `.md` file.
- **Working definition of “session”:** A single temporally bounded interaction, usually daily.
- **Memory boundary types:** Three are in play — Decision Cycle, Decision Step, Session.
- **RAG search methods:** Cletho’s MVP will support keyword, semantic, and metadata filtering.
- **Backend database:** Supabase selected, with emphasis on clear boundary between user vs system data.
- **Session journaling method:** Rick will upload the prior day’s `.md` file at session start.
- **Session file management:** ChatGPT will integrate past context by processing the most recent file and threading from there.

---

## 3. Key Previous Decisions

- **Division of system components:**  
  1. Known/standard (e.g. frontend/backend infra).  
  2. Documented/unfamiliar (e.g. RAG stack).  
  3. Novel/custom (e.g. Cletho’s decision-partnering behavior).  

- **Cletho's RAG usage:** RAG will be invoked both in response to user questions and as part of scripted Cletho prompts.

- **Embedding session context:** Leaning toward using a distilled summary rather than raw turn history for context embedding, but undecided.

- **Source corpus:** Cletho will retrieve only from curated, internal knowledge bases — no live external APIs.

---

## 4. Key RAG Design Considerations and Constraints

- Cletho, not the user, is the active agent in most query flows — requiring RAG queries to be generated internally.
- A classifier is likely needed to distinguish among user statements, clarifying questions, and RAG-triggering queries.
- Session summarization is likely needed for memory persistence and efficient semantic search.
- The retrieval corpus will include multiple types of knowledge (domain, procedural, examples) and must be tagged accordingly.
- Clear separation between user-generated data (e.g. session transcripts, decision content) and system data (e.g. BK, TK, EK) is required.

---

## 5. Full Transcript of Session

### Combined transcript from 2025-08-25 and 2025-08-26:

## Project Planning Dialogue — Minimum Viable RAG for Cletho

---

### Rick:

> I need you to help me make a project plan to build a minimum viable RAG system in the next seven weeks. Let me lay out the assumptions and whatnot, and we'll take this one step at a time.

> In looking at the entire system and how it's going to work — which is somewhat still vague right now — Jeff and I have divided it into three categories:  
> 
> 1. Things we know how to do and have done before, such as standard front end and back end.  
> 2. Things we have not done before, but for which there seems to be plenty of documentation and even available code to tweak and use, such as a RAG system.  
> 3. Things that are brand new, with no real frame of reference, such as a Cletho construct that asks questions and deftly draws information out of a user that the user can use to make rational decisions.  
>
> That last item is definitely the hardest part, and we are going to sit down for a month of working side-by-side on that, starting in about seven weeks. That part is the construction and interpretation of the decision matrix, which will be a 6–7 step process. The plan is to build a minimum viable first set of steps, then incrementally build them out in a spiral fashion. The goal there is just a proof of concept. To do that, we need a minimum viable RAG.
>
> Now, having just finished the Coursera RAG course, I can see how a RAG normally works to augment user prompts. In the case of Cletho — where it's Cletho asking the questions (or prompting) and processing the answers to construct the next prompt, not the user — it's not clear how it's going to work.

---

### ChatGPT:

**Excellent framing — thank you.**  
That three-part breakdown (1: familiar engineering, 2: documented but new territory, 3: novel design challenges) is a strong scaffold for this seven-week push. Let me reflect back the core intent as I understand it, then suggest the structure of a project plan, before we jointly define Week 1.

---

### 🔍 Clarifying Your Project Goal

Your immediate aim is to build a **Minimum Viable RAG system** that can serve the needs of **Cletho’s decision-making dialogue engine**. You want this RAG system to be:

- ✅ Functional enough to demonstrate how retrieval can support a multi-turn, system-initiated dialogue,  
- ✅ Minimal enough that it doesn’t overcommit you to implementation details you haven’t yet finalized for the “Cletho Core,”  
- ✅ Robust enough that when you and Jeff start your deep design sprint in ~7 weeks, you’re not held back by infra, data prep, or brittle code.

Right?

---

### 🛠️ Nature of the Challenge

You're building a **system-prompted RAG pipeline** — not a user-prompted one.  
This means:

| Standard RAG             | Cletho's Variant             |
|--------------------------|------------------------------|
| User asks a question     | Cletho asks a question       |
| System retrieves docs    | System retrieves docs        |
| System generates response| User provides the response   |
| System continues dialogue| Cletho generates the next Q  |

**Which raises open questions:**

- How will the RAG system know what to retrieve if the user isn’t driving the query?  
- Should it retrieve based on Cletho’s current question, or based on user’s last response?  
- Or should the entire session so far be used as a context window for retrieval?

These aren’t academic — your RAG MVP needs to make one of these assumptions explicitly, and live with it (for now).

---

### 📆 Seven-Week Spiral Project Plan

I propose a four-phase plan:

| Phase              | Timeframe   | Goal                                                |
|-------------------|-------------|-----------------------------------------------------|
| 1. Baseline        | Week 1      | Simple user-prompted RAG with local corpus          |
| 2. Cletho Mode 1   | Weeks 2–3   | RAG pulls docs based on Cletho’s question           |
| 3. Cletho Mode 2   | Weeks 4–5   | RAG pulls docs based on user responses              |
| 4. Integration Prep| Weeks 6–7   | Hardened module for decision matrix integration     |

We now focus on **Week 1** planning. The goal is to ensure you have:

- A local corpus (e.g., markdown or JSON docs)  
- A working retriever (FAISS or Weaviate-lite)  
- An LLM wrapper (OpenAI for now)  
- A CLI or notebook demo showing:  
  → question → retrieval → response  

**Key constraint:**  
The system should be **modular** so that swapping “user question” with “Cletho question” in Week 2 is trivial.

---

### 🔧 What I Need from You Now

To define the Week 1 milestone and checklist, I need to know:

1. **What stack/tools do you want to use for MVP RAG?**  
   - FAISS? Weaviate? LangChain? LlamaIndex? Just OpenAI + NumPy?

2. **What kind of documents will be used in the MVP corpus?**  
   - Markdown? Web scrapes? Cletho's internal docs?

3. **Where will this run?**  
   - Jupyter? Local script? Streamlit prototype?

4. **What retrieval granularity?**  
   - Full doc? Paragraph? Sentence?

---

> Go ahead. Let’s get precise.
